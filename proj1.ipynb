{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Convolutional Neural Networks (CNNs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch packages\n",
    "import torch\n",
    "import torchvision as torchv\n",
    "\n",
    "# Packages that are nice to have\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchv.datasets.CIFAR10(\n",
    "    root='./dataset/train',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchv.transforms.ToTensor()\n",
    ")\n",
    "test_dataset = torchv.datasets.CIFAR10(\n",
    "    root='./dataset/test',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchv.transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the data look like?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.2314, 0.1686, 0.1961,  ..., 0.6196, 0.5961, 0.5804],\n",
      "         [0.0627, 0.0000, 0.0706,  ..., 0.4824, 0.4667, 0.4784],\n",
      "         [0.0980, 0.0627, 0.1922,  ..., 0.4627, 0.4706, 0.4275],\n",
      "         ...,\n",
      "         [0.8157, 0.7882, 0.7765,  ..., 0.6275, 0.2196, 0.2078],\n",
      "         [0.7059, 0.6784, 0.7294,  ..., 0.7216, 0.3804, 0.3255],\n",
      "         [0.6941, 0.6588, 0.7020,  ..., 0.8471, 0.5922, 0.4824]],\n",
      "\n",
      "        [[0.2431, 0.1804, 0.1882,  ..., 0.5176, 0.4902, 0.4863],\n",
      "         [0.0784, 0.0000, 0.0314,  ..., 0.3451, 0.3255, 0.3412],\n",
      "         [0.0941, 0.0275, 0.1059,  ..., 0.3294, 0.3294, 0.2863],\n",
      "         ...,\n",
      "         [0.6667, 0.6000, 0.6314,  ..., 0.5216, 0.1216, 0.1333],\n",
      "         [0.5451, 0.4824, 0.5647,  ..., 0.5804, 0.2431, 0.2078],\n",
      "         [0.5647, 0.5059, 0.5569,  ..., 0.7216, 0.4627, 0.3608]],\n",
      "\n",
      "        [[0.2471, 0.1765, 0.1686,  ..., 0.4235, 0.4000, 0.4039],\n",
      "         [0.0784, 0.0000, 0.0000,  ..., 0.2157, 0.1961, 0.2235],\n",
      "         [0.0824, 0.0000, 0.0314,  ..., 0.1961, 0.1961, 0.1647],\n",
      "         ...,\n",
      "         [0.3765, 0.1333, 0.1020,  ..., 0.2745, 0.0275, 0.0784],\n",
      "         [0.3765, 0.1647, 0.1176,  ..., 0.3686, 0.1333, 0.1333],\n",
      "         [0.4549, 0.3686, 0.3412,  ..., 0.5490, 0.3294, 0.2824]]]), 6)\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(type(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single sample of the data is a tuple of length 2. This will be the input into our convolutional neural network!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# First element of tuple. This is an image!\n",
    "print(train_dataset[0][0].shape)\n",
    "print(type(train_dataset[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may not be familiar with what a \"tensor\" is as of now. Just think of it as a datatype similar to an array!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at a single sample of the second element. This will be our label (or \"target\" in PyTorch) to compare the output of our network to in order to determine the loss or error of our network so we can tell the network how to improve the network.\n",
    "\n",
    "Keep in mind this is can also be called the ground-truth of the input sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Second element is just a single integer!\n",
    "print(train_dataset[0][1])\n",
    "print(type(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are these two elements of the single sample?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first element is an image of shape 1x28x28 (NumberOfChanngels \\* Width \\* Height) representing a single handwritten digit (i.e. a number between 0 and 9).\n",
    "- The second element is a single digit integer representing the handwritten digit's perceived value contained within the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first element of the tuple, we see that the shape of the image is 1x28x28. Why do we have the single 1 in the front? This is the number of color channels the image has in it. In this case, we only have one color channel conveying that this image is simply a gray scale image!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q1: How many color channels does a traditional colored image have?_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that this is only ONE sample of the dataset!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50000 samples in the training dataset!\n",
      "There are 10000 samples in the test dataset!\n",
      "Thus we have 60000 samples in total!\n",
      "That is, we have a total of 60000 tuples containing a single grayscale image and a single digit integer!\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(train_dataset)} samples in the training dataset!')\n",
    "print(f'There are {len(test_dataset)} samples in the test dataset!')\n",
    "print(\n",
    "    f'Thus we have {len(train_dataset) + len(test_dataset)} ' +\n",
    "    'samples in total!\\n' +\n",
    "    f'That is, we have a total of {len(train_dataset) + len(test_dataset)} ' +\n",
    "    'tuples containing a single grayscale image and a single digit integer!'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and Split the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We currently have two separate parts of the dataset: the training data and the testing data. For this example we will combine these two into one large dataset of 70,000 samples and then split the dataset into three new datasets: train, validation, and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images: (80, 32, 32, 3)\n",
      "Train Labels: (80,)\n",
      "Test Images: (20, 32, 32, 3)\n",
      "Test Labels: (20,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the data into numpy arrays\n",
    "# (convert data to numpy format to manipulate later)\n",
    "train_images = train_dataset.data[0:80]\n",
    "train_labels = np.array(train_dataset.targets[0:80])\n",
    "test_images = test_dataset.data[0:20]\n",
    "test_labels = np.array(test_dataset.targets[0:20])\n",
    "print(\n",
    "    f'Train Images: {train_images.shape}\\n' +\n",
    "    f'Train Labels: {train_labels.shape}\\n' +\n",
    "    f'Test Images: {test_images.shape}\\n' +\n",
    "    f'Test Labels: {test_labels.shape}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Images: (100, 32, 32, 3)\n",
      "All Labels: (100,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine the train and test dataset into one big dataset\n",
    "all_images = np.concatenate((train_images, test_images))\n",
    "all_labels = np.concatenate((train_labels, test_labels))\n",
    "print(\n",
    "    f'All Images: {all_images.shape}\\n' +\n",
    "    f'All Labels: {all_labels.shape}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice! That the color channel dimension of 1 was removed! This is fine for now. We will add it back later!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have one large dataset, we split the data into the proportions we wish to use for our three separate datasets for training, validation, and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 20, 10)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the large numpy array into smaller train, validation, and test splits\n",
    "choices = np.arange(len(all_labels))\n",
    "# Specify the percentages of samples each set should contain\n",
    "# NOTE: The last percentage is not important here since we will just\n",
    "#       use the remaining images after we take out the training and validation\n",
    "#       sets.\n",
    "train_perc, val_perc, test_perc = (0.7, 0.2, 0.1)\n",
    "# Get the number of total samples\n",
    "num_samples = len(all_labels)\n",
    "# Calculate the number train samples we want\n",
    "num_train = int(np.floor(num_samples * train_perc))\n",
    "# Calculate the number of validation samples we want\n",
    "num_val = int(np.floor(num_samples * val_perc))\n",
    "# Calculate the number of test samples we want\n",
    "num_test = num_samples - num_train - num_val\n",
    "# Show the number of samples in each\n",
    "num_train, num_val, num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([87, 94, 39, 10, 82, 47, 44, 75, 50, 34, 38, 26, 86, 45, 89, 27, 40,\n",
       "        58, 56, 84, 31, 71, 69, 81,  6, 65, 90, 73, 12, 37, 62, 33, 63, 16,\n",
       "        24, 29, 57, 36, 98, 78, 21, 72, 88, 43, 54, 48, 23, 20,  9, 53,  1,\n",
       "        79, 92, 61,  7, 19, 76, 15, 67, 97, 83, 52, 66, 95, 51, 80,  5, 30,\n",
       "        74, 64]),\n",
       " array([35, 91, 28, 42, 99, 70,  4,  0, 77, 22, 18, 41, 11, 68,  8, 96,  2,\n",
       "        60, 17, 14]),\n",
       " array([ 3, 13, 25, 32, 46, 49, 55, 59, 85, 93]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly select indices throughout the whole dataset\n",
    "train_idx = np.random.choice(choices, num_train, replace=False)\n",
    "# Get the set difference between the whole dataset and the chosen training\n",
    "# indices\n",
    "choices = np.setdiff1d(choices, train_idx)\n",
    "# Now get randomly choose the validation indices\n",
    "val_idx = np.random.choice(choices, num_val, replace=False)\n",
    "# Similarly, get the set difference but this time the resulting difference\n",
    "# is, in fact, the test set.\n",
    "test_idx = np.setdiff1d(choices, val_idx)\n",
    "# Show index sets\n",
    "train_idx, val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure disjoint sets i.e. none of the elements overlap between the new\n",
    "# three datasets of train, validation, and test.\n",
    "np.intersect1d(np.intersect1d(train_idx, val_idx), test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we actually get all of the images and labels corresponding to their\n",
    "# sets using the indices we randomly chose\n",
    "train_images, train_labels = all_images[train_idx], all_labels[train_idx]\n",
    "val_images, val_labels = all_images[val_idx], all_labels[val_idx]\n",
    "test_images, test_labels = all_images[test_idx], all_labels[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions and Custom Dataset Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(image):\n",
    "    \"\"\"Perform a transform on an input image. This can include normalization,\n",
    "    padding, and other transformations/augmentations.\n",
    "\n",
    "    :param image: The input image.\n",
    "    :type image: Typically a numpy array.\n",
    "    :return: A new transformed image.\n",
    "    :rtype: _type_\n",
    "    \"\"\"\n",
    "    # I used ChatGPT to help me with this part\n",
    "    x = image.transpose(2, 0, 1)      # (3, 32, 32)\n",
    "    x = torch.from_numpy(x).float() / 255.0\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def target_transform(label):\n",
    "    \"\"\"Perform transformations on the label (i.e. \"target\").\n",
    "\n",
    "    :param label: An input integer in this case\n",
    "    :type label: A number that we should typecast to integer.\n",
    "    :return: A transformed label.\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    x = int(label)\n",
    "    return x\n",
    "\n",
    "\n",
    "class CIFAR10_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Create a custom PyTorch dataset with all of the necessary functions to\n",
    "    properly work with other PyTorch operations/functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        images,\n",
    "        labels,\n",
    "        transform=transform,\n",
    "        target_transform=target_transform\n",
    "    ):\n",
    "        \"\"\"The constructor for the class. Initialize variables.\n",
    "\n",
    "        :param images: A group of images.\n",
    "        :param labels: A group of labels.\n",
    "        :param transform: Transform function to apply to images, \n",
    "            defaults to transform\n",
    "        :type transform: function, optional\n",
    "        :param target_transform: Transform function to apply to the labels,\n",
    "            defaults to target_transform\n",
    "        :type target_transform: function, optional\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the length of the dataset.\n",
    "\n",
    "        :return: The length of the labels i.e. the number of elements in the\n",
    "            dataset.\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a single element of the dataset via its index and perform the\n",
    "        necessary transforms upon it.\n",
    "\n",
    "        :param idx: The index of the element to retrieve.\n",
    "        :type idx: int\n",
    "        :return: A tuple grouping the image and the label.\n",
    "        :rtype: tuple\n",
    "        \"\"\"\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our custom dataset definition class to wrap our dataset for use with other\n",
    "# PyTorch tools i.e. the PyTorch dataloader.\n",
    "train_dataset = CIFAR10_Dataset(\n",
    "    train_images, train_labels, transform=transform, target_transform=target_transform)\n",
    "val_dataset = CIFAR10_Dataset(\n",
    "    val_images, val_labels, transform=transform, target_transform=target_transform)\n",
    "test_dataset = CIFAR10_Dataset(\n",
    "    test_images, test_labels, transform=transform, target_transform=target_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pause and analyze our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 9 9 4 1 1 2 7 8 3 4 7 7 2 9 9 9 3 2 6 4 3 6 6 2 6 3 5 4 0 0 9 1 3 4 0 3\n",
      " 7 3 3 5 2 2 7 1 1 1 2 2 0 9 5 7 9 2 2 5 2 4 3 1 1 8 2 1 1 4 9 7 8 5 9 6 7\n",
      " 3 1 9 0 3 1 3 8 8 0 6 6 1 6 3 1 0 9 5 7 9 8 5 7 8 6]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram showing the frequency of each class in the dataset\n",
    "print(all_labels)\n",
    "plt.hist(all_labels, bins=np.arange(11) - 0.5, edgecolor='black')\n",
    "plt.xticks(range(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have mixed distribution of frequencies among the 10 classes of handwritten digits with handwritten 1s having the most samples and the 5s having the least samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q2: What happens if our distribution is very unbalanced meaning we have widely different quantities of the classes?_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we look at the distribution over the train, validation, and test sets, we can see that the distributions are roughly proportional to what we saw previously when the all of the data was contained within a single large dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x702c5a18aec0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Frequencies of each class in the 3 separate datasets\n",
    "labels = np.unique(train_labels)\n",
    "plt.hist([train_labels, val_labels, test_labels])\n",
    "plt.xticks(labels)\n",
    "plt.legend(['Train', 'Val', 'Test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get an idea of what our data actually looks like. Here, we have printed a single sample of each class and displayed their corresponding label above them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvisualization_samples = []\\nplt.subplot(2, 5, 1)\\nfor i in range(10):\\n    indices = np.where(train_labels == i)[0]\\n    rand_idx = indices[np.random.randint(0, len(indices) - 1)]\\n    plt.subplot(2, 5, i+1)\\n    plt.imshow(np.squeeze(train_images[rand_idx]))\\n    plt.yticks([])\\n    plt.xticks([])\\n    plt.title(i)\\n    '"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying one sample of each class\n",
    "\"\"\"\n",
    "visualization_samples = []\n",
    "plt.subplot(2, 5, 1)\n",
    "for i in range(10):\n",
    "    indices = np.where(train_labels == i)[0]\n",
    "    rand_idx = indices[np.random.randint(0, len(indices) - 1)]\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(np.squeeze(train_images[rand_idx]))\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    plt.title(i)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataloaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases our entire dataset may not fit entirely in memory (i.e. RAM). In our case of MNIST, this typically isn't a problem, but for larger datasets and higher dimensional data (i.e. ImageNet datasets), it is common that we need a way to load our data such that it fits in memory.\n",
    "\n",
    "We can customize the code in the MNIST_Dataset class to allow use to only read data from the disk (Hard Drive or Solid State Drive) when needed. This can be seen in the PyTorch examples of the Dataset and Dataloader documentation/tutorials: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files\n",
    "\n",
    "In this case, we are not concerned with memory limitations, unless the system you are running this tutorial on is extremely limited in memory. Thus we have the entire dataset loaded into memory and can freely manipulate it.\n",
    "\n",
    "In many cases, it is much more efficient to use data batching to batch our data when training to speed up training. Instead of inputting a single image, computing the loss, and then backpropogating the error and adjusting the weights, we can perform the forward pass (i.e. input) several images at once and then compute the loss on those samples together and then backpropogate their error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q3: What do we mean by \"train faster\" or \"more efficiently\" here? That is what is faster or more efficient?_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this tutorial, we will use a batch size of 32 whilst training\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we initialize the Dataloaders, set the batch size to load BATCH_SIZE\n",
    "# samples at a time and also perform a random shuffle of the samples within\n",
    "# each set of data.\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the CNN Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we instantiate two CNN models both being based off of Yann LeCun et. al.'s LeNet-5 CNN proposed in their publication \"Gradient-Based Learning Applied to Document Recognition\" circa 1998.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 10 different classes of data we want to differentiate between\n",
    "# (i.e. the handwritten single digit number zero thru nine)\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a close-to-classical version of LeNet-5 which is very similar to what Yann LeCun et. al. proposed in 1998.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define the layers\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        self.maxpool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        self.maxpool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = torch.nn.Conv2d(\n",
    "            in_channels=128,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        self.conv4 = torch.nn.Conv2d(\n",
    "            in_channels=128,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "\n",
    "        self.maxpool3 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear1 = torch.nn.Linear(128, 100)\n",
    "        self.linear2 = torch.nn.Linear(100, 75)\n",
    "        self.linear3 = torch.nn.Linear(75, 50)\n",
    "        self.linear4 = torch.nn.Linear(50, 35)\n",
    "        self.linear5 = torch.nn.Linear(35, out_features=NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.functional.F.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = torch.functional.F.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = torch.functional.F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = torch.functional.F.relu(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        # print(f\"shape of flat vector {x.shape}\")\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = torch.functional.F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = torch.functional.F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = torch.functional.F.relu(x)\n",
    "        x = self.linear4(x)\n",
    "        x = torch.functional.F.softmax(x)\n",
    "        x = self.linear5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_images[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Original LeNet-5 Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1454/71479964.py:69: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = torch.functional.F.softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VGGModel                                 [32, 10]                  --\n",
       "Conv2d: 1-1                            [32, 64, 30, 30]          1,792\n",
       "MaxPool2d: 1-2                         [32, 64, 15, 15]          --\n",
       "Conv2d: 1-3                            [32, 128, 13, 13]         73,856\n",
       "MaxPool2d: 1-4                         [32, 128, 6, 6]           --\n",
       "Conv2d: 1-5                            [32, 128, 4, 4]           147,584\n",
       "Conv2d: 1-6                            [32, 128, 2, 2]           147,584\n",
       "MaxPool2d: 1-7                         [32, 128, 1, 1]           --\n",
       "Flatten: 1-8                           [32, 128]                 --\n",
       "Linear: 1-9                            [32, 100]                 12,900\n",
       "Linear: 1-10                           [32, 75]                  7,575\n",
       "Linear: 1-11                           [32, 50]                  3,800\n",
       "Linear: 1-12                           [32, 35]                  1,785\n",
       "Linear: 1-13                           [32, 10]                  360\n",
       "==========================================================================================\n",
       "Total params: 397,236\n",
       "Trainable params: 397,236\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 546.32\n",
       "==========================================================================================\n",
       "Input size (MB): 0.39\n",
       "Forward/backward pass size (MB): 21.01\n",
       "Params size (MB): 1.59\n",
       "Estimated Total Size (MB): 22.99\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model = VGGModel()\n",
    "summary(vgg_model, (BATCH_SIZE, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q4: What are the architectural differences between these two models?_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Loss Functions and Optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paremeters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our learning rate for both models\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup our loss function\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Setup an optimizer for each of the two models\n",
    "optim_vgg = torch.optim.Adam(\n",
    "    vgg_model.parameters(), lr=LEARNING_RATE)\n",
    "# optim_modern_lenet5 = torch.optim.Adam(\n",
    "    # modern_lenet5.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the CNN Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of training epochs to loop of the entire dataset\n",
    "NUM_EPOCHS = 5\n",
    "# Determine if we should use\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(outputs, labels):\n",
    "    \"\"\"Use the outputs from the network and the expected labels to compute the\n",
    "    accuracy of the network. \n",
    "\n",
    "    :param outputs: The output provided by running the model on input.\n",
    "    :param labels: The ground-truth labels of the dataset.\n",
    "    :return: The accuracy of the network based on the output of the network.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    # Get the class index with the highest activation\n",
    "    # In this case it corresponds to the digit with the highest likelihood of\n",
    "    # being the number depicted in the input\n",
    "    predictions = torch.argmax(outputs, 1)\n",
    "    # Number of total predictions (i.e. the batch size)\n",
    "    num_predictions = len(predictions)\n",
    "    # Count the number of elements in the difference that are NOT zero\n",
    "    # This indicates a mis-classification by the model\n",
    "    num_incorrect = torch.count_nonzero(predictions - labels)\n",
    "    # Compute the accuracy i.e. total - incorrect divided by total\n",
    "    accuracy = (num_predictions - num_incorrect)/num_predictions\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    loss_function,\n",
    "    optimizer,\n",
    "    epoch: int,\n",
    "    num_epochs: int,\n",
    "    total_steps: int,\n",
    "    device: str\n",
    ") -> tuple:\n",
    "    \"\"\"Perform a full iteration over the entire training dataset.\n",
    "\n",
    "    :param train_dataloader: The dataloader to use to get the data.\n",
    "    :type train_dataloader: torch.utils.data.DataLoader\n",
    "    :param model: The model to train.\n",
    "    :type model: torch.nn.Module\n",
    "    :param loss_function: The loss function to use to compute the loss.\n",
    "    :param optimizer: The optimizer to use during training.\n",
    "    :param epoch: The epoch number.\n",
    "    :type epoch: int\n",
    "    :param num_epochs: Total number of epochs.\n",
    "    :type num_epochs: int\n",
    "    :param total_steps: Total number of loops due to batching.\n",
    "    :type total_steps: int\n",
    "    :param device: The device to train on.\n",
    "    :type device: str\n",
    "    :return: A tuple containing average loss and accuracy.\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    running_acc = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "        # Move the data to the desired training device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Perform the forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        # Compute accuracy metric\n",
    "        acc = compute_accuracy(outputs, labels)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform the backwards pass (i.e. backpropogate the error)\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add the running accuracy and loss\n",
    "        running_loss += loss.item()\n",
    "        running_acc += acc\n",
    "\n",
    "        # Progressively print loss\n",
    "        if (i+1) % 256 == 0:\n",
    "            print(\n",
    "                f'TRAINING --> Epoch: {epoch+1}/{num_epochs}, ' +\n",
    "                f'Step: {i+1}/{total_steps}, ' +\n",
    "                f'Loss: {running_loss / (i+1)}, '\n",
    "                f'Accuracy: {running_acc / (i+1)}'\n",
    "            )\n",
    "    # Compute the average loss and accuracy for this epoch\n",
    "    running_loss = running_loss / total_steps\n",
    "    running_acc = running_acc / total_steps\n",
    "\n",
    "    return running_loss, running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(\n",
    "    val_dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    loss_function,\n",
    "    epoch: int,\n",
    "    num_epochs: int,\n",
    "    total_steps: int,\n",
    "    device: str\n",
    ") -> tuple:\n",
    "    \"\"\"Perform inference over the entire validation dataset without training\n",
    "    the model.\n",
    "\n",
    "    :param val_dataloader: The validation dataset DataLoader to get data.\n",
    "    :type val_dataloader: torch.utils.data.DataLoader\n",
    "    :param model: The model to use to perform inference.\n",
    "    :type model: torch.nn.Module\n",
    "    :param loss_function: The loss function to use to compute the loss with.\n",
    "    :param epoch: The current epoch number.\n",
    "    :type epoch: int\n",
    "    :param num_epochs: Total number of epochs.\n",
    "    :type num_epochs: int\n",
    "    :param total_steps: Total number of loop iteration due to batching.\n",
    "    :type total_steps: int\n",
    "    :param device: The device to perform inference on.\n",
    "    :type device: str\n",
    "    :return: The average validation loss and accuracy.\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    # Now we run over the validation dataset without training\n",
    "    if val_dataloader:\n",
    "        # Disable the gradient calculations and updates\n",
    "        with torch.no_grad():\n",
    "            for i, (images, labels) in enumerate(val_dataloader):\n",
    "                # Transfer input data to device\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Perform inference\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Compute validation loss\n",
    "                loss = loss_function(outputs, labels)\n",
    "\n",
    "                # Compute validation accuracy\n",
    "                acc = compute_accuracy(outputs, labels)\n",
    "\n",
    "                # Add the running accuracy and loss\n",
    "                running_loss += loss.item()\n",
    "                running_acc += acc\n",
    "\n",
    "                # Progressively print loss\n",
    "                if (i+1) % 256 == 0:\n",
    "                    print(\n",
    "                        f'VALIDATION --> Epoch: {epoch+1}/{num_epochs}, ' +\n",
    "                        f'Step: {i+1}/{total_steps}, ' +\n",
    "                        f'Val Loss: {running_loss / (i+1)}, ' +\n",
    "                        f'Val Acc: {running_acc / (i+1)}'\n",
    "                    )\n",
    "    # Compute the average loss and accuracy for this epoch\n",
    "    running_loss = running_loss / total_steps\n",
    "    running_acc = running_acc / total_steps\n",
    "\n",
    "    return running_loss, running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(\n",
    "    model: torch.nn.Module,\n",
    "    model_save_path: Path,\n",
    "    val_loss: float,\n",
    "    val_losses: list,\n",
    "    epoch: int,\n",
    "    keep_models: bool = False\n",
    "):\n",
    "    \"\"\"Save the model if it is the first epoch. Subsequently, save the model\n",
    "    only if a lower validation loss is achieved whilst training.\n",
    "\n",
    "    :param model: The model to save.\n",
    "    :type model: torch.nn.Module\n",
    "    :param model_save_path: The location to save the model to.\n",
    "    :type model_save_path: Path\n",
    "    :param val_loss: The current epoch's validation loss.\n",
    "    :type val_loss: float\n",
    "    :param val_losses: The history of all other validation losses.\n",
    "    :type val_losses: list\n",
    "    :param epoch: The current epoch number.\n",
    "    :type epoch: int\n",
    "    :param keep_models: Should all models be saved, defaults to False\n",
    "    :type keep_models: bool, optional\n",
    "    \"\"\"\n",
    "    # Should we keep all models or just one\n",
    "    if keep_models:\n",
    "        model_save_path = model_save_path / f'model_{epoch+1}_{val_loss}.pt'\n",
    "    else:\n",
    "        model_save_path = model_save_path / f'model_state_dict.pt'\n",
    "    # Save the first model\n",
    "    if len(val_losses) == 0:\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            model_save_path\n",
    "        )\n",
    "        print(\n",
    "            'SAVING --> First epoch: \\n' +\n",
    "            f'Val Loss: {val_loss}\\n' +\n",
    "            f'Saving new model to:\\n{model_save_path}'\n",
    "        )\n",
    "    elif val_loss < min(val_losses):\n",
    "        # If our new validation loss is less than the previous best save the\n",
    "        # model\n",
    "        print(\n",
    "            'SAVING --> Found model with better validation loss: \\n' +\n",
    "            f'New Best Val Loss: {val_loss}\\n' +\n",
    "            f'Old Best Val Loss: {min(val_losses)}\\n'\n",
    "            f'Saving new model to:\\n{model_save_path}'\n",
    "        )\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            model_save_path\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    loss_function: torch.nn.CrossEntropyLoss,\n",
    "    optimizer: torch.optim.Adam,\n",
    "    num_epochs: int,\n",
    "    device: str,\n",
    "    model_save_path: Path = Path('./models'),\n",
    "    val_dataloader: torch.utils.data.DataLoader = None,\n",
    ") -> tuple:\n",
    "    \"\"\"Training loop which iterates over every image in the training dataset,\n",
    "    performs the forward pass, computes the loss, and then performs the\n",
    "    backwards pass. This also loops over the validation dataset and computes\n",
    "    the validation loss and accuracy to determine the best model.\n",
    "\n",
    "    See: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "    :param train_dataloader: The training dataset DataLoader to get data.\n",
    "    :type train_dataloader: torch.utils.data.DataLoader\n",
    "    :param model: The model to train.\n",
    "    :type model: torch.nn.Module\n",
    "    :param loss_function: The loss function to use to compute the loss.\n",
    "    :type loss_function: torch.nn.CrossEntropyLoss\n",
    "    :param optimizer: The optimizer to use in updating the weights.\n",
    "    :type optimizer: torch.optim.Adam\n",
    "    :param num_epochs: Total number of epoch to train for.\n",
    "    :type num_epochs: int\n",
    "    :param device: The device to perform training and validation on.\n",
    "    :type device: str\n",
    "    :param model_save_path: Save the model to, defaults to Path('./models')\n",
    "    :type model_save_path: Path, optional\n",
    "    :param val_dataloader: The validation dataset loader, defaults to None\n",
    "    :type val_dataloader: torch.utils.data.DataLoader, optional\n",
    "    :return: A tuple of shape (2, 2) that contains the history of the train\n",
    "        and validation losses and accuracies.\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    print(f'Models will be saved to: {model_save_path}')\n",
    "    # Lists for recording stats over epochs\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "    # Create the save path for the model\n",
    "    if not model_save_path.exists():\n",
    "        model_save_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Total number batches in training set\n",
    "    train_total_steps = len(train_dataloader)\n",
    "    val_total_steps = len(val_dataloader)\n",
    "\n",
    "    # Perform training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Enable model training\n",
    "        model.train(True)\n",
    "\n",
    "        # Enter the training function loop\n",
    "        train_loss, train_acc = train(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            num_epochs,\n",
    "            train_total_steps,\n",
    "            device\n",
    "        )\n",
    "        print(\n",
    "            f'TRAINING --> Epoch {epoch+1}/{NUM_EPOCHS} DONE, ' +\n",
    "            f'Avg Loss: {train_loss}, Avg Accuracy: {train_acc}'\n",
    "        )\n",
    "\n",
    "        # Enter the validation loop\n",
    "        val_loss, val_acc = validation(\n",
    "            val_dataloader,\n",
    "            model,\n",
    "            loss_function,\n",
    "            epoch,\n",
    "            num_epochs,\n",
    "            val_total_steps,\n",
    "            device\n",
    "        )\n",
    "        print(\n",
    "            f'VALIDATION --> Epoch {epoch+1}/{NUM_EPOCHS} DONE, ' +\n",
    "            f'Avg Loss: {val_loss}, Avg Accuracy: {val_acc}'\n",
    "        )\n",
    "\n",
    "        # Determine if we should save the model\n",
    "        save_best_model(model, model_save_path, val_loss, val_losses, epoch)\n",
    "\n",
    "        # Record the stats\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "    return (train_losses, train_accs), (val_losses, val_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q5: What is the purpose of the loss function?_**\n",
    "\n",
    "**_Q6: What is validation doing?_**\n",
    "\n",
    "**_Q7: Why do we perform validation and not just use the training loss/accuracy?_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epoch_metrics(x, y, data_names, title_prefix, yaxis_label):\n",
    "    \"\"\"Plot metrics with the number of epochs on the x axis and the metric of\n",
    "    interest on the y axis. Note that this function differs based on the input.\n",
    "\n",
    "    :param x: The values to use on the x-axis.\n",
    "    :type x: list\n",
    "    :param y: A list of lists containing len(x) data points to plot. The inner\n",
    "        lists are the different series to plot.\n",
    "    :type y: list\n",
    "    :param data_names: Names of the series to use in the legend.\n",
    "    :type data_names: str\n",
    "    :param title_prefix: A prefix to add before everything else in the title.\n",
    "    :type title_prefix: str\n",
    "    :param yaxis_label: The label for the y axis.\n",
    "    :type yaxis_label: str\n",
    "    \"\"\"\n",
    "    # Plot multiple series of data\n",
    "    for i in y:\n",
    "        plt.plot(x, i)\n",
    "    # Set the title\n",
    "    plt.title(title_prefix + ' ' + ' vs. '.join(data_names) + ' ' + yaxis_label)\n",
    "    # Set the y axis label\n",
    "    plt.ylabel(yaxis_label)\n",
    "    # Enable the legend with the appropriate names\n",
    "    plt.legend(data_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models will be saved to: models/vgg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1454/71479964.py:69: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = torch.functional.F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING --> Epoch 1/5 DONE, Avg Loss: 2.288567543029785, Avg Accuracy: 0.10416666666666667\n",
      "VALIDATION --> Epoch 1/5 DONE, Avg Loss: 2.2964539527893066, Avg Accuracy: 0.15000000596046448\n",
      "SAVING --> First epoch: \n",
      "Val Loss: 2.2964539527893066\n",
      "Saving new model to:\n",
      "models/vgg/model_state_dict.pt\n",
      "TRAINING --> Epoch 2/5 DONE, Avg Loss: 2.294456720352173, Avg Accuracy: 0.1944444477558136\n",
      "VALIDATION --> Epoch 2/5 DONE, Avg Loss: 2.2968010902404785, Avg Accuracy: 0.15000000596046448\n",
      "TRAINING --> Epoch 3/5 DONE, Avg Loss: 2.301767905553182, Avg Accuracy: 0.14930555721124014\n",
      "VALIDATION --> Epoch 3/5 DONE, Avg Loss: 2.296905040740967, Avg Accuracy: 0.15000000596046448\n",
      "TRAINING --> Epoch 4/5 DONE, Avg Loss: 2.2990763982137046, Avg Accuracy: 0.14930555721124014\n",
      "VALIDATION --> Epoch 4/5 DONE, Avg Loss: 2.2962124347686768, Avg Accuracy: 0.15000000596046448\n",
      "SAVING --> Found model with better validation loss: \n",
      "New Best Val Loss: 2.2962124347686768\n",
      "Old Best Val Loss: 2.2964539527893066\n",
      "Saving new model to:\n",
      "models/vgg/model_state_dict.pt\n",
      "TRAINING --> Epoch 5/5 DONE, Avg Loss: 2.2883547147115073, Avg Accuracy: 0.14930555721124014\n",
      "VALIDATION --> Epoch 5/5 DONE, Avg Loss: 2.298002004623413, Avg Accuracy: 0.15000000596046448\n",
      "Best Validation Loss: 2.2962124347686768 after epoch 4\n",
      "Best Validation Acc: 0.15000000596046448 after epoch 1\n"
     ]
    }
   ],
   "source": [
    "# Train the VGG Model\n",
    "vgg_model.to(DEVICE)\n",
    "(train_losses, train_accs), (val_losses, val_accs) = train_model(\n",
    "    train_dataloader,\n",
    "    vgg_model,\n",
    "    loss,\n",
    "    optim_vgg,\n",
    "    NUM_EPOCHS,\n",
    "    DEVICE,\n",
    "    model_save_path=Path('./models/vgg'),\n",
    "    val_dataloader=val_dataloader\n",
    ")\n",
    "# Get the best validation loss and accuracy\n",
    "print(\n",
    "    f'Best Validation Loss: {min(val_losses)} ' +\n",
    "    f'after epoch {np.argmin(val_losses) + 1}'\n",
    ")\n",
    "print(\n",
    "    f'Best Validation Acc: {max(val_accs)} ' +\n",
    "    f'after epoch {np.argmax(val_accs) + 1}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (10,) and (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_epoch_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_losses\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mValidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOriginal LeNet-5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLoss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[105], line 19\u001b[0m, in \u001b[0;36mplot_epoch_metrics\u001b[0;34m(x, y, data_names, title_prefix, yaxis_label)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Plot multiple series of data\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m y:\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Set the title\u001b[39;00m\n\u001b[1;32m     21\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(title_prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m vs. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data_names) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m yaxis_label)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/pyplot.py:3838\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3830\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3831\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3832\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3836\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3837\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3839\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3842\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3843\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3844\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1777\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1536\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1776\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1777\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_base.py:297\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_base.py:494\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    491\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    495\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (10,) and (5,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_epoch_metrics(\n",
    "    np.arange(10),\n",
    "    [train_losses, val_losses],\n",
    "    ['Train', 'Validation'],\n",
    "    'Original LeNet-5',\n",
    "    'Loss'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epoch_metrics(\n",
    "    np.arange(10),\n",
    "    [train_accs, val_accs],\n",
    "    ['Train', 'Validation'],\n",
    "    'Original LeNet-5',\n",
    "    'Accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q8: Do you think training the models for additional epochs will help in any way? Explain your answer!_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our model and chosen the best model based on the validation loss, let's test our saved models and compare them in terms of their loss and accuracy on the test dataset. If the models have been trained well, we should see they perform decently well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(labels, predictions):\n",
    "    \"\"\"This function generates a confusion matrix which shows how the model\n",
    "    performs over the different classes in the dataset. In other words, this\n",
    "    shows which classes the model 'confuses' with one another.\n",
    "\n",
    "    :param labels: The ground-truth labels.\n",
    "    :type labels: list\n",
    "    :param predictions: The predictions provided by the model.\n",
    "    :type predictions: list\n",
    "    \"\"\"\n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    # Create a matplotlib display for the confusion matrix\n",
    "    cm_display = ConfusionMatrixDisplay(cm, display_labels=np.arange(10))\n",
    "    # Plot the data\n",
    "    cm_display.plot()\n",
    "    # SHow the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    model_class: torch.nn.Module,\n",
    "    model_weights_path: Path,\n",
    "    device: str,\n",
    "    loss_function\n",
    "):\n",
    "    \"\"\"Load the saved model and evaluate it using the test dataset to determine\n",
    "    how well it was trained.\n",
    "\n",
    "    :param test_dataloader: The test dataset DataLoader to get the data from.\n",
    "    :type test_dataloader: torch.utils.data.DataLoader\n",
    "    :param model_class: The model to evaluate.\n",
    "    :type model_class: torch.nn.Module\n",
    "    :param model_weights_path: The path the the model's weights.\n",
    "    :type model_weights_path: Path\n",
    "    :param device: The device to use for evaluation (i.e. inference).\n",
    "    :type device: str\n",
    "    :param loss_function: The loss function to use to compute the test loss.\n",
    "    :return: A tuple containing the total average loss and accuracy as well as\n",
    "        two list containing the predictions and ground-truth labels.\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    # Initialize the model architecture\n",
    "    model: torch.nn.Module = model_class()\n",
    "    # Load the weights\n",
    "    model.load_state_dict(torch.load(model_weights_path, weights_only=True))\n",
    "    # Enter the model into evaluation mode (i.e. DO NOT TRAIN)\n",
    "    model.eval()\n",
    "\n",
    "    # Send the model to the device to use for inference\n",
    "    model.to(device)\n",
    "    # Number of batched iterations\n",
    "    total_steps = len(test_dataloader)\n",
    "    # Vars for metrics and results\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    running_latency = 0\n",
    "    # Testing/Evaluation loop\n",
    "    for i, (images, labels) in enumerate(test_dataloader):\n",
    "        # Send data to the inference device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Perform inference on batched data\n",
    "        begin_time = time.time()\n",
    "        outputs = model(images)\n",
    "        end_time = time.time()\n",
    "        running_latency += end_time - begin_time\n",
    "        \n",
    "\n",
    "        # Compute the loss (error) between the output and the ground-truths\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        # Compute the accuracy\n",
    "        accuracy = compute_accuracy(outputs, labels)\n",
    "\n",
    "        # Keep a running loss and accuracy for computing the average\n",
    "        running_loss += loss.item()\n",
    "        running_acc += accuracy\n",
    "\n",
    "        # Concatenate all of the predictions and ground-truths\n",
    "        all_predictions = all_predictions + torch.argmax(outputs, 1).tolist()\n",
    "        all_labels = all_labels + labels.tolist()\n",
    "    # Compute the average loss and accuracy\n",
    "    running_loss = running_loss / total_steps\n",
    "    running_acc = running_acc / total_steps\n",
    "    avg_latency = running_latency / total_steps\n",
    "\n",
    "    return running_loss, running_acc, all_predictions, all_labels, avg_latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original LeNet-5 Inference Performance\n",
      "Test dataset contains 10 samples.\n",
      "Testing Avg Loss: 2.3356661796569824\n",
      "Testing Avg Acc: 0.0\n",
      "Testing Avg Latency: 0.0535 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1454/71479964.py:69: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = torch.functional.F.softmax(x)\n"
     ]
    }
   ],
   "source": [
    "og_test_loss, og_test_acc, predictions, labels, avg_latency = test_model(\n",
    "    test_dataloader,\n",
    "    VGGModel,\n",
    "    './models/vgg/model_state_dict.pt',\n",
    "    DEVICE,\n",
    "    loss\n",
    ")\n",
    "print('Original LeNet-5 Inference Performance')\n",
    "print(f'Test dataset contains {len(test_dataset)} samples.')\n",
    "print(f'Testing Avg Loss: {og_test_loss}')\n",
    "print(f'Testing Avg Acc: {og_test_acc}')\n",
    "print(f\"Testing Avg Latency: {round(avg_latency, 4)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of FixedLocator locations (8), usually from a call to set_ticks, does not match the number of labels (10).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_confusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[108], line 16\u001b[0m, in \u001b[0;36mplot_confusion_matrix\u001b[0;34m(labels, predictions)\u001b[0m\n\u001b[1;32m     14\u001b[0m cm_display \u001b[38;5;241m=\u001b[39m ConfusionMatrixDisplay(cm, display_labels\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Plot the data\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mcm_display\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# SHow the figure\u001b[39;00m\n\u001b[1;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_plot/confusion_matrix.py:188\u001b[0m, in \u001b[0;36mConfusionMatrixDisplay.plot\u001b[0;34m(self, include_values, cmap, xticks_rotation, values_format, ax, colorbar, im_kw, text_kw)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m colorbar:\n\u001b[1;32m    187\u001b[0m     fig\u001b[38;5;241m.\u001b[39mcolorbar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim_, ax\u001b[38;5;241m=\u001b[39max)\n\u001b[0;32m--> 188\u001b[0m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxticks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43myticks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxticklabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43myticklabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mylabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrue label\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPredicted label\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim((n_classes \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m))\n\u001b[1;32m    198\u001b[0m plt\u001b[38;5;241m.\u001b[39msetp(ax\u001b[38;5;241m.\u001b[39mget_xticklabels(), rotation\u001b[38;5;241m=\u001b[39mxticks_rotation)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/artist.py:146\u001b[0m, in \u001b[0;36mArtist.__init_subclass__.<locals>.<lambda>\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_autogenerated_signature\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# Don't overwrite cls.set if the subclass or one of its parents\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# has defined a set method set itself.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# If there was no explicit definition, cls.set is inherited from\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# the hierarchy of auto-generated set methods, which hold the\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# flag _autogenerated_signature.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mArtist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/artist.py:1241\u001b[0m, in \u001b[0;36mArtist.set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1238\u001b[0m     \u001b[38;5;66;03m# docstring and signature are auto-generated via\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m     \u001b[38;5;66;03m# Artist._update_set_signature_and_docstring() at the end of the\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# module.\u001b[39;00m\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/artist.py:1233\u001b[0m, in \u001b[0;36mArtist._internal_update\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_internal_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, kwargs):\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;124;03m    Update artist properties without prenormalizing them, but generating\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;124;03m    errors as if calling `set`.\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m \n\u001b[1;32m   1231\u001b[0m \u001b[38;5;124;03m    The lack of prenormalization is to maintain backcompatibility.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_props\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{cls.__name__}\u001b[39;49;00m\u001b[38;5;124;43m.set() got an unexpected keyword argument \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{prop_name!r}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/artist.py:1209\u001b[0m, in \u001b[0;36mArtist._update_props\u001b[0;34m(self, props, errfmt)\u001b[0m\n\u001b[1;32m   1205\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func):\n\u001b[1;32m   1206\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1207\u001b[0m                     errfmt\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), prop_name\u001b[38;5;241m=\u001b[39mk),\n\u001b[1;32m   1208\u001b[0m                     name\u001b[38;5;241m=\u001b[39mk)\n\u001b[0;32m-> 1209\u001b[0m             ret\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpchanged()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_base.py:74\u001b[0m, in \u001b[0;36m_axis_method_wrapper.__set_name__.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axis.py:2106\u001b[0m, in \u001b[0;36mAxis.set_ticklabels\u001b[0;34m(self, labels, minor, fontdict, **kwargs)\u001b[0m\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(locator, mticker\u001b[38;5;241m.\u001b[39mFixedLocator):\n\u001b[1;32m   2103\u001b[0m     \u001b[38;5;66;03m# Passing [] as a list of labels is often used as a way to\u001b[39;00m\n\u001b[1;32m   2104\u001b[0m     \u001b[38;5;66;03m# remove all tick labels, so only error for > 0 labels\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(locator\u001b[38;5;241m.\u001b[39mlocs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(labels) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2106\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of FixedLocator locations\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2108\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(locator\u001b[38;5;241m.\u001b[39mlocs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), usually from a call to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set_ticks, does not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2110\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the number of labels (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2111\u001b[0m     tickd \u001b[38;5;241m=\u001b[39m {loc: lab \u001b[38;5;28;01mfor\u001b[39;00m loc, lab \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(locator\u001b[38;5;241m.\u001b[39mlocs, labels)}\n\u001b[1;32m   2112\u001b[0m     func \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_with_dict, tickd)\n",
      "\u001b[0;31mValueError\u001b[0m: The number of FixedLocator locations (8), usually from a call to set_ticks, does not match the number of labels (10)."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q9: How does the test loss and accuracy compare to the best training and validation metrics?_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q10: How do you think we could compute the latency inside the `test_model` function? In other words, what code do we add to `test_model` to measure the time it takes to give the model input and then get the output?_**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
