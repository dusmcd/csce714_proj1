{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Convolutional Neural Networks (CNNs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch packages\n",
    "import torch\n",
    "import torchvision as torchv\n",
    "\n",
    "# Packages that are nice to have\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchv.datasets.CIFAR10(\n",
    "    root='./dataset/train',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchv.transforms.ToTensor()\n",
    ")\n",
    "test_dataset = torchv.datasets.CIFAR10(\n",
    "    root='./dataset/test',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchv.transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the data look like?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.2314, 0.1686, 0.1961,  ..., 0.6196, 0.5961, 0.5804],\n",
      "         [0.0627, 0.0000, 0.0706,  ..., 0.4824, 0.4667, 0.4784],\n",
      "         [0.0980, 0.0627, 0.1922,  ..., 0.4627, 0.4706, 0.4275],\n",
      "         ...,\n",
      "         [0.8157, 0.7882, 0.7765,  ..., 0.6275, 0.2196, 0.2078],\n",
      "         [0.7059, 0.6784, 0.7294,  ..., 0.7216, 0.3804, 0.3255],\n",
      "         [0.6941, 0.6588, 0.7020,  ..., 0.8471, 0.5922, 0.4824]],\n",
      "\n",
      "        [[0.2431, 0.1804, 0.1882,  ..., 0.5176, 0.4902, 0.4863],\n",
      "         [0.0784, 0.0000, 0.0314,  ..., 0.3451, 0.3255, 0.3412],\n",
      "         [0.0941, 0.0275, 0.1059,  ..., 0.3294, 0.3294, 0.2863],\n",
      "         ...,\n",
      "         [0.6667, 0.6000, 0.6314,  ..., 0.5216, 0.1216, 0.1333],\n",
      "         [0.5451, 0.4824, 0.5647,  ..., 0.5804, 0.2431, 0.2078],\n",
      "         [0.5647, 0.5059, 0.5569,  ..., 0.7216, 0.4627, 0.3608]],\n",
      "\n",
      "        [[0.2471, 0.1765, 0.1686,  ..., 0.4235, 0.4000, 0.4039],\n",
      "         [0.0784, 0.0000, 0.0000,  ..., 0.2157, 0.1961, 0.2235],\n",
      "         [0.0824, 0.0000, 0.0314,  ..., 0.1961, 0.1961, 0.1647],\n",
      "         ...,\n",
      "         [0.3765, 0.1333, 0.1020,  ..., 0.2745, 0.0275, 0.0784],\n",
      "         [0.3765, 0.1647, 0.1176,  ..., 0.3686, 0.1333, 0.1333],\n",
      "         [0.4549, 0.3686, 0.3412,  ..., 0.5490, 0.3294, 0.2824]]]), 6)\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(type(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single sample of the data is a tuple of length 2. This will be the input into our convolutional neural network!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# First element of tuple. This is an image!\n",
    "print(train_dataset[0][0].shape)\n",
    "print(type(train_dataset[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may not be familiar with what a \"tensor\" is as of now. Just think of it as a datatype similar to an array!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at a single sample of the second element. This will be our label (or \"target\" in PyTorch) to compare the output of our network to in order to determine the loss or error of our network so we can tell the network how to improve the network.\n",
    "\n",
    "Keep in mind this is can also be called the ground-truth of the input sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Second element is just a single integer!\n",
    "print(train_dataset[0][1])\n",
    "print(type(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are these two elements of the single sample?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first element is an image of shape 1x28x28 (NumberOfChanngels \\* Width \\* Height) representing a single handwritten digit (i.e. a number between 0 and 9).\n",
    "- The second element is a single digit integer representing the handwritten digit's perceived value contained within the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first element of the tuple, we see that the shape of the image is 1x28x28. Why do we have the single 1 in the front? This is the number of color channels the image has in it. In this case, we only have one color channel conveying that this image is simply a gray scale image!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q1: How many color channels does a traditional colored image have?_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that this is only ONE sample of the dataset!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50000 samples in the training dataset!\n",
      "There are 10000 samples in the test dataset!\n",
      "Thus we have 60000 samples in total!\n",
      "That is, we have a total of 60000 tuples containing a single grayscale image and a single digit integer!\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(train_dataset)} samples in the training dataset!')\n",
    "print(f'There are {len(test_dataset)} samples in the test dataset!')\n",
    "print(\n",
    "    f'Thus we have {len(train_dataset) + len(test_dataset)} ' +\n",
    "    'samples in total!\\n' +\n",
    "    f'That is, we have a total of {len(train_dataset) + len(test_dataset)} ' +\n",
    "    'tuples containing a single grayscale image and a single digit integer!'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and Split the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We currently have two separate parts of the dataset: the training data and the testing data. For this example we will combine these two into one large dataset of 70,000 samples and then split the dataset into three new datasets: train, validation, and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images: (1000, 32, 32, 3)\n",
      "Train Labels: (1000,)\n",
      "Test Images: (100, 32, 32, 3)\n",
      "Test Labels: (100,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the data into numpy arrays\n",
    "# (convert data to numpy format to manipulate later)\n",
    "train_images = train_dataset.data\n",
    "train_labels = np.array(train_dataset.targets)\n",
    "test_images = test_dataset.data\n",
    "test_labels = np.array(test_dataset.targets)\n",
    "print(\n",
    "    f'Train Images: {train_images.shape}\\n' +\n",
    "    f'Train Labels: {train_labels.shape}\\n' +\n",
    "    f'Test Images: {test_images.shape}\\n' +\n",
    "    f'Test Labels: {test_labels.shape}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Images: (1100, 32, 32, 3)\n",
      "All Labels: (1100,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine the train and test dataset into one big dataset\n",
    "all_images = np.concatenate((train_images, test_images))\n",
    "all_labels = np.concatenate((train_labels, test_labels))\n",
    "print(\n",
    "    f'All Images: {all_images.shape}\\n' +\n",
    "    f'All Labels: {all_labels.shape}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice! That the color channel dimension of 1 was removed! This is fine for now. We will add it back later!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have one large dataset, we split the data into the proportions we wish to use for our three separate datasets for training, validation, and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(770, 220, 110)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the large numpy array into smaller train, validation, and test splits\n",
    "choices = np.arange(len(all_labels))\n",
    "# Specify the percentages of samples each set should contain\n",
    "# NOTE: The last percentage is not important here since we will just\n",
    "#       use the remaining images after we take out the training and validation\n",
    "#       sets.\n",
    "train_perc, val_perc, test_perc = (0.7, 0.2, 0.1)\n",
    "# Get the number of total samples\n",
    "num_samples = len(all_labels)\n",
    "# Calculate the number train samples we want\n",
    "num_train = int(np.floor(num_samples * train_perc))\n",
    "# Calculate the number of validation samples we want\n",
    "num_val = int(np.floor(num_samples * val_perc))\n",
    "# Calculate the number of test samples we want\n",
    "num_test = num_samples - num_train - num_val\n",
    "# Show the number of samples in each\n",
    "num_train, num_val, num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 932,  624,  177,  850,   23,  813,  140,  953,  962,  370,  730,\n",
       "         234,  710,  388,  205,  892,  990,  313,  831,   64,  785,  181,\n",
       "         505,  912,  702,  927,  642,  584,  635,  391,   96,  339,   41,\n",
       "         336,  992, 1064, 1059,  582,  937,  490,  724,   75,  671,  673,\n",
       "         950,   99,  318,  413,  420,  108,  109,  910,  257,  287,  282,\n",
       "         903,  501,  217, 1025, 1078,  754, 1040,    4,  676,  964,  829,\n",
       "        1004,  253,  100,  264,  779,  466,  900,   90,  296,  652,  803,\n",
       "         312,   34,  649,  678,  562,    9,  498,  665,  147,  263,  404,\n",
       "         133,  835,  841,  273,  704,  827,  235,  978,  204,  107,  252,\n",
       "          30,  129,  419, 1028,  856,  764,  350,  290, 1097,  761,  979,\n",
       "         488,  902,  382,  124,  314, 1069,  638,  943,  601,  987,  777,\n",
       "          49,  102, 1065, 1021,  748,  356,  171,  579,  343,  465,  332,\n",
       "         759,  882,  533,  862,  424,  593,  144,  767,   45,  131,  165,\n",
       "         223, 1046,    5,  110,  457,  139,  403,  914,  548,  657,  895,\n",
       "         989, 1073,  654,  551,  685,  869,  961, 1075,  202,  843,  410,\n",
       "         507, 1001,  489,  500,  787,  737,   20,    7,  746,  297,  135,\n",
       "         365,  959,  320,  630,  201,  768,  575,  606,  619,  508, 1063,\n",
       "         141,  137,  407, 1060,  691,  820,  664,  319,  732,  812,  997,\n",
       "         174,   79,  183,  275,  988,  231,  666,  672, 1043,  647,  999,\n",
       "         743,  499,  958,  246,   15,  569,  675,  776,  514,  185,  483,\n",
       "         708,  583,  170, 1008,  766,  592,  733,   57,  265,  837,  238,\n",
       "         434,  154,   94,  605,  928,  686,  590,  711, 1085,  239, 1051,\n",
       "        1020,  556,  536,  427,  426,  311,   68,  492,  736, 1002,  946,\n",
       "          52, 1049,  833,  656,  464,  353,   38,   71,  479, 1056,  521,\n",
       "        1093, 1005,  214,   17,  397,  106,  225,  639,  415, 1098,  773,\n",
       "          21,  832,  119,  333,  662,   46, 1077,  522,  526,  369,  518,\n",
       "         744,  378,  258,  753, 1055,  819,  237,  374, 1070,  974,   13,\n",
       "          12,  998,   65,  127,  190,   37,  111,  781,  458,  640,  884,\n",
       "         344,  317,  411,  756,  975,  168,  729,  417,  476,  797,   73,\n",
       "         865,  565,  248,  855, 1030,  159,  890,  920,  271,  405,  581,\n",
       "         825, 1050, 1044,    1,  535,  603, 1022,  823,  965, 1081,  199,\n",
       "         194,  491,  321,  985,  623,   59,  968,  698,  770,  529,  469,\n",
       "         323,  634,  936,  555,  534,  380,  784,  879,  778,  760,  616,\n",
       "         695,  256,  907,  716,  864,  598,  502,  966,  728,  815,  316,\n",
       "         539,  160,  358,  169,  270,  838,  478,  881,  347,    3,  104,\n",
       "         352,  738,  206,  149,  524,  122,  432,  189, 1026,  991,  462,\n",
       "         299, 1083,  560,  971,  162,  463,  422,  577,  653,  148,  915,\n",
       "         279, 1092,  158,  292,  852, 1086,  918,  302,  908,  166,  612,\n",
       "        1088,  694,  607,  693,  633,  786,  816,  289,  342,  445,  117,\n",
       "         103,  146,  898, 1023,  554,  367,  916, 1029,  487,  418,  689,\n",
       "         442,  337,  136,   50,   42,  269,  674,  281,  810,    6,  429,\n",
       "         532,  994,  215,  125,  254,  294,  794,  880,  620,  519,  437,\n",
       "         398,  788,  406,  349,  373,  451,   69,  970,  969,  834,  385,\n",
       "         443,  952,  300,   29,  613,  955,  150,  949,  574,  922,  295,\n",
       "         924,  718,  707,  818,  485,  173,  175,  847,   16, 1053,  722,\n",
       "         546,  706,  940,  116,  283,  394,  917,  286,  983,  973,  242,\n",
       "         930,   81,  995,  811,  925,  368,  301, 1006,  941,  857,  272,\n",
       "         774,  742,  659,  152,  247,  176,  471,   95,   35,  516,  482,\n",
       "         375,  885,  804,   24,  926,  240,  717,  473,  243,  957,  188,\n",
       "         266,   92,  566,  901,  372,  541,  699,  967,  288,  184,   51,\n",
       "         101,  494,  762,    0,  597,  330,  697,  280,  126,  221,   91,\n",
       "         859,  128,  627,  360,  875,  824,  933,  860,  452,  572,  195,\n",
       "         455,  143,   25,   97,  444,  559,  609,  629,  408,  669,  921,\n",
       "         851, 1032,  345, 1079,  447,  956,  573,  808,  359,  324,  757,\n",
       "         120,  913,  431,  230, 1042,  800,  911,  430,  783,  806,  363,\n",
       "         877,  648,  576,  749, 1074,  381,  872,  180,  545,  355,  715,\n",
       "         769,  423, 1094,  538, 1031,  543,  421,  660, 1024,  503,  366,\n",
       "         696,  591,  735,  362,  346,  267,  198,  853,  981,  796,  433,\n",
       "         212,  530,  745,  435,   48, 1015, 1003,  705,  622,  340,  392,\n",
       "          47,  495,  899,  496,  984,  771,  327,  883,  200,  982,  255,\n",
       "         497,  567,  475,  886,  772,  977,  703, 1057,  661,  632,   54,\n",
       "         599,  396,  747,  512,  376,  643, 1087,  679,  276,   78,  132,\n",
       "          77,  677,  791,  752,  809,  335, 1041,  112,  531,  414,  822,\n",
       "        1047,  142, 1052,  887,  121,  523,  600,  739,  996,  527,  719,\n",
       "         755,  663,   14,  621,  720,  291,  219, 1089,   60,  341,  790,\n",
       "         904,  826,   11,  734,  114,   74,  364,  580,  203,   36,  210,\n",
       "         828,  894,   32,  504,  618,  453,  389,  658,  211,  751, 1013,\n",
       "         383,  459,  608,   33,  844,  179,  589,  259,  449,  935,  260,\n",
       "         446,  650,  474,  224,   70,  938,  293,  610,  552, 1039,  303,\n",
       "         726,  963,  113,  456,  155,  690,  400,  792, 1045,  617,   86]),\n",
       " array([ 954,  439,  986,  167,  542,  585,  261,  390,  655, 1019,  450,\n",
       "         305, 1062,  701, 1072,  229,  309,  644,  611,  326,  285,  218,\n",
       "         848,   44,  425,    2,   61,  145,  571,  944,   53,    8,  945,\n",
       "         191,  241,  192, 1033,  511,  680,  960,   43,  251,  976,  595,\n",
       "         481,  561,  588, 1018,  153,  845,  750, 1090,  520,  436,   87,\n",
       "         763,  448,  568,  328,   80,  931,  830,  278,  249,  712,  798,\n",
       "         731, 1036,   72,  454,  244,  187,  161,  840, 1017,  615,   26,\n",
       "         513,  493,  896,  371,  178, 1066,  631,  682,  846,  789,  713,\n",
       "         727,  758,  277,  315,  222, 1099,  307,  878,  891,  156,  874,\n",
       "         993,  870,   63,  262,  386,  182,  637,  354,  821, 1080,  972,\n",
       "         351,   18, 1091,  409,  668,  645,  227,  723,  460,  213,  947,\n",
       "         334, 1082,  393,  438,  412,  651,  849,  268, 1095,  782,  684,\n",
       "         934, 1084,  322,  540,  306,   67,  951,   56,  377,  814,  889,\n",
       "          82, 1000,  486, 1038,  228,  387,  793,  646,  379,  905,  506,\n",
       "         594,  868,  893,  740,  480,  641,  614,  839,  357,  164,   66,\n",
       "         528,  207,  440,   58,  384,  468,  134,  151,  888, 1068,   27,\n",
       "         775,  799, 1027,  667,  331,  525,  138,  596,  836,  807,  193,\n",
       "         329,  220,  681,  586,  163,   28,  557,  578,  304,  399, 1048,\n",
       "         692,  537, 1016,  549,  802,  510,  587,  570, 1058,  906,  980,\n",
       "         517,  876,  801,  348,  416,  897,  741,  604,  602,  338,   39]),\n",
       " array([  10,   19,   22,   31,   40,   55,   62,   76,   83,   84,   85,\n",
       "          88,   89,   93,   98,  105,  115,  118,  123,  130,  157,  172,\n",
       "         186,  196,  197,  208,  209,  216,  226,  232,  233,  236,  245,\n",
       "         250,  274,  284,  298,  308,  310,  325,  361,  395,  401,  402,\n",
       "         428,  441,  461,  467,  470,  472,  477,  484,  509,  515,  544,\n",
       "         547,  550,  553,  558,  563,  564,  625,  626,  628,  636,  670,\n",
       "         683,  687,  688,  700,  709,  714,  721,  725,  765,  780,  795,\n",
       "         805,  817,  842,  854,  858,  861,  863,  866,  867,  871,  873,\n",
       "         909,  919,  923,  929,  939,  942,  948, 1007, 1009, 1010, 1011,\n",
       "        1012, 1014, 1034, 1035, 1037, 1054, 1061, 1067, 1071, 1076, 1096]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly select indices throughout the whole dataset\n",
    "train_idx = np.random.choice(choices, num_train, replace=False)\n",
    "# Get the set difference between the whole dataset and the chosen training\n",
    "# indices\n",
    "choices = np.setdiff1d(choices, train_idx)\n",
    "# Now get randomly choose the validation indices\n",
    "val_idx = np.random.choice(choices, num_val, replace=False)\n",
    "# Similarly, get the set difference but this time the resulting difference\n",
    "# is, in fact, the test set.\n",
    "test_idx = np.setdiff1d(choices, val_idx)\n",
    "# Show index sets\n",
    "train_idx, val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure disjoint sets i.e. none of the elements overlap between the new\n",
    "# three datasets of train, validation, and test.\n",
    "np.intersect1d(np.intersect1d(train_idx, val_idx), test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we actually get all of the images and labels corresponding to their\n",
    "# sets using the indices we randomly chose\n",
    "train_images, train_labels = all_images[train_idx], all_labels[train_idx]\n",
    "val_images, val_labels = all_images[val_idx], all_labels[val_idx]\n",
    "test_images, test_labels = all_images[test_idx], all_labels[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions and Custom Dataset Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(image):\n",
    "    \"\"\"Perform a transform on an input image. This can include normalization,\n",
    "    padding, and other transformations/augmentations.\n",
    "\n",
    "    :param image: The input image.\n",
    "    :type image: Typically a numpy array.\n",
    "    :return: A new transformed image.\n",
    "    :rtype: _type_\n",
    "    \"\"\"\n",
    "    x = np.reshape(image, (3, 32, 32))\n",
    "    x = torch.Tensor(x / 255.0)\n",
    "    return x\n",
    "\n",
    "\n",
    "def target_transform(label):\n",
    "    \"\"\"Perform transformations on the label (i.e. \"target\").\n",
    "\n",
    "    :param label: An input integer in this case\n",
    "    :type label: A number that we should typecast to integer.\n",
    "    :return: A transformed label.\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    x = int(label)\n",
    "    return x\n",
    "\n",
    "\n",
    "class CIFAR10_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Create a custom PyTorch dataset with all of the necessary functions to\n",
    "    properly work with other PyTorch operations/functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        images,\n",
    "        labels,\n",
    "        transform=transform,\n",
    "        target_transform=target_transform\n",
    "    ):\n",
    "        \"\"\"The constructor for the class. Initialize variables.\n",
    "\n",
    "        :param images: A group of images.\n",
    "        :param labels: A group of labels.\n",
    "        :param transform: Transform function to apply to images, \n",
    "            defaults to transform\n",
    "        :type transform: function, optional\n",
    "        :param target_transform: Transform function to apply to the labels,\n",
    "            defaults to target_transform\n",
    "        :type target_transform: function, optional\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the length of the dataset.\n",
    "\n",
    "        :return: The length of the labels i.e. the number of elements in the\n",
    "            dataset.\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a single element of the dataset via its index and perform the\n",
    "        necessary transforms upon it.\n",
    "\n",
    "        :param idx: The index of the element to retrieve.\n",
    "        :type idx: int\n",
    "        :return: A tuple grouping the image and the label.\n",
    "        :rtype: tuple\n",
    "        \"\"\"\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our custom dataset definition class to wrap our dataset for use with other\n",
    "# PyTorch tools i.e. the PyTorch dataloader.\n",
    "train_dataset = CIFAR10_Dataset(\n",
    "    train_images, train_labels, transform=transform, target_transform=target_transform)\n",
    "val_dataset = CIFAR10_Dataset(\n",
    "    val_images, val_labels, transform=transform, target_transform=target_transform)\n",
    "test_dataset = CIFAR10_Dataset(\n",
    "    test_images, test_labels, transform=transform, target_transform=target_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pause and analyze our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 9 9 ... 0 0 7]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram showing the frequency of each class in the dataset\n",
    "print(all_labels)\n",
    "plt.hist(all_labels, bins=np.arange(11) - 0.5, edgecolor='black')\n",
    "plt.xticks(range(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have mixed distribution of frequencies among the 10 classes of handwritten digits with handwritten 1s having the most samples and the 5s having the least samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q2: What happens if our distribution is very unbalanced meaning we have widely different quantities of the classes?_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we look at the distribution over the train, validation, and test sets, we can see that the distributions are roughly proportional to what we saw previously when the all of the data was contained within a single large dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x71e771cf3010>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Frequencies of each class in the 3 separate datasets\n",
    "labels = np.unique(train_labels)\n",
    "plt.hist([train_labels, val_labels, test_labels])\n",
    "plt.xticks(labels)\n",
    "plt.legend(['Train', 'Val', 'Test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get an idea of what our data actually looks like. Here, we have printed a single sample of each class and displayed their corresponding label above them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying one sample of each class\n",
    "\"\"\"\n",
    "visualization_samples = []\n",
    "plt.subplot(2, 5, 1)\n",
    "for i in range(10):\n",
    "    indices = np.where(train_labels == i)[0]\n",
    "    rand_idx = indices[np.random.randint(0, len(indices) - 1)]\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(np.squeeze(train_images[rand_idx]))\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    plt.title(i)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataloaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases our entire dataset may not fit entirely in memory (i.e. RAM). In our case of MNIST, this typically isn't a problem, but for larger datasets and higher dimensional data (i.e. ImageNet datasets), it is common that we need a way to load our data such that it fits in memory.\n",
    "\n",
    "We can customize the code in the MNIST_Dataset class to allow use to only read data from the disk (Hard Drive or Solid State Drive) when needed. This can be seen in the PyTorch examples of the Dataset and Dataloader documentation/tutorials: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files\n",
    "\n",
    "In this case, we are not concerned with memory limitations, unless the system you are running this tutorial on is extremely limited in memory. Thus we have the entire dataset loaded into memory and can freely manipulate it.\n",
    "\n",
    "In many cases, it is much more efficient to use data batching to batch our data when training to speed up training. Instead of inputting a single image, computing the loss, and then backpropogating the error and adjusting the weights, we can perform the forward pass (i.e. input) several images at once and then compute the loss on those samples together and then backpropogate their error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q3: What do we mean by \"train faster\" or \"more efficiently\" here? That is what is faster or more efficient?_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this tutorial, we will use a batch size of 32 whilst training\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we initialize the Dataloaders, set the batch size to load BATCH_SIZE\n",
    "# samples at a time and also perform a random shuffle of the samples within\n",
    "# each set of data.\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the CNN Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we instantiate two CNN models both being based off of Yann LeCun et. al.'s LeNet-5 CNN proposed in their publication \"Gradient-Based Learning Applied to Document Recognition\" circa 1998.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 10 different classes of data we want to differentiate between\n",
    "# (i.e. the handwritten single digit number zero thru nine)\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a close-to-classical version of LeNet-5 which is very similar to what Yann LeCun et. al. proposed in 1998.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define the layers\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=16,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        self.maxpool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        self.maxpool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = torch.nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        self.conv4 = torch.nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "\n",
    "        self.maxpool3 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # self.conv5 = torch.nn.Conv2d(\n",
    "        #     in_channels=64,\n",
    "        #     out_channels=128,\n",
    "        #     kernel_size=3,\n",
    "        #     stride=1,\n",
    "        #     padding=0\n",
    "        # )\n",
    "        # self.conv6 = torch.nn.Conv2d(\n",
    "        #     in_channels=128,\n",
    "        #     out_channels=128,\n",
    "        #     kernel_size=3,\n",
    "        #     stride=1,\n",
    "        #     padding=0\n",
    "        # )\n",
    "\n",
    "        # self.maxpool4 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # self.conv7 = torch.nn.Conv2d(\n",
    "        #     in_channels=128,\n",
    "        #     out_channels=256,\n",
    "        #     kernel_size=3,\n",
    "        #     stride=1,\n",
    "        #     padding=0\n",
    "        # )\n",
    "        # self.conv8 = torch.nn.Conv2d(\n",
    "        #     in_channels=256,\n",
    "        #     out_channels=256,\n",
    "        #     kernel_size=3,\n",
    "        #     stride=1,\n",
    "        #     padding=0\n",
    "        # )\n",
    "\n",
    "        # self.maxpool5 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear1 = torch.nn.Linear(64, 32)\n",
    "        self.linear2 = torch.nn.Linear(32, 16)\n",
    "        self.linear3 = torch.nn.Linear(16, out_features=NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.functional.F.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = torch.functional.F.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = torch.functional.F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = torch.functional.F.relu(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        # x = self.conv5(x)\n",
    "        # x = torch.functional.F.relu(x)\n",
    "        # x = self.conv6(x)\n",
    "        # x = torch.functional.F.relu(x)\n",
    "        # x = self.maxpool4(x)\n",
    "\n",
    "        # x = self.conv7(x)\n",
    "        # x = torch.functional.F.relu(x)\n",
    "        # x = self.conv8(x)\n",
    "        # x = torch.functional.F.relu(x)\n",
    "        # x = self.maxpool5(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = torch.functional.F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = torch.functional.F.softmax(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_images[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Original LeNet-5 Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of flat vector: torch.Size([32, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24955/394743340.py:106: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = torch.functional.F.softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VGGModel                                 [32, 10]                  --\n",
       "Conv2d: 1-1                            [32, 16, 30, 30]          448\n",
       "MaxPool2d: 1-2                         [32, 16, 15, 15]          --\n",
       "Conv2d: 1-3                            [32, 32, 13, 13]          4,640\n",
       "MaxPool2d: 1-4                         [32, 32, 6, 6]            --\n",
       "Conv2d: 1-5                            [32, 64, 4, 4]            18,496\n",
       "Conv2d: 1-6                            [32, 64, 2, 2]            36,928\n",
       "MaxPool2d: 1-7                         [32, 64, 1, 1]            --\n",
       "Flatten: 1-8                           [32, 64]                  --\n",
       "Linear: 1-9                            [32, 32]                  2,080\n",
       "Linear: 1-10                           [32, 16]                  528\n",
       "Linear: 1-11                           [32, 10]                  170\n",
       "==========================================================================================\n",
       "Total params: 63,290\n",
       "Trainable params: 63,290\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 52.28\n",
       "==========================================================================================\n",
       "Input size (MB): 0.39\n",
       "Forward/backward pass size (MB): 5.41\n",
       "Params size (MB): 0.25\n",
       "Estimated Total Size (MB): 6.06\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model = VGGModel()\n",
    "summary(vgg_model, (BATCH_SIZE, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q4: What are the architectural differences between these two models?_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Loss Functions and Optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paremeters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our learning rate for both models\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup our loss function\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Setup an optimizer for each of the two models\n",
    "optim_vgg = torch.optim.Adam(\n",
    "    vgg_model.parameters(), lr=LEARNING_RATE)\n",
    "# optim_modern_lenet5 = torch.optim.Adam(\n",
    "    # modern_lenet5.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the CNN Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of training epochs to loop of the entire dataset\n",
    "NUM_EPOCHS = 10\n",
    "# Determine if we should use\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(outputs, labels):\n",
    "    \"\"\"Use the outputs from the network and the expected labels to compute the\n",
    "    accuracy of the network. \n",
    "\n",
    "    :param outputs: The output provided by running the model on input.\n",
    "    :param labels: The ground-truth labels of the dataset.\n",
    "    :return: The accuracy of the network based on the output of the network.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    # Get the class index with the highest activation\n",
    "    # In this case it corresponds to the digit with the highest likelihood of\n",
    "    # being the number depicted in the input\n",
    "    predictions = torch.argmax(outputs, 1)\n",
    "    # Number of total predictions (i.e. the batch size)\n",
    "    num_predictions = len(predictions)\n",
    "    # Count the number of elements in the difference that are NOT zero\n",
    "    # This indicates a mis-classification by the model\n",
    "    num_incorrect = torch.count_nonzero(predictions - labels)\n",
    "    # Compute the accuracy i.e. total - incorrect divided by total\n",
    "    accuracy = (num_predictions - num_incorrect)/num_predictions\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    loss_function,\n",
    "    optimizer,\n",
    "    epoch: int,\n",
    "    num_epochs: int,\n",
    "    total_steps: int,\n",
    "    device: str\n",
    ") -> tuple:\n",
    "    \"\"\"Perform a full iteration over the entire training dataset.\n",
    "\n",
    "    :param train_dataloader: The dataloader to use to get the data.\n",
    "    :type train_dataloader: torch.utils.data.DataLoader\n",
    "    :param model: The model to train.\n",
    "    :type model: torch.nn.Module\n",
    "    :param loss_function: The loss function to use to compute the loss.\n",
    "    :param optimizer: The optimizer to use during training.\n",
    "    :param epoch: The epoch number.\n",
    "    :type epoch: int\n",
    "    :param num_epochs: Total number of epochs.\n",
    "    :type num_epochs: int\n",
    "    :param total_steps: Total number of loops due to batching.\n",
    "    :type total_steps: int\n",
    "    :param device: The device to train on.\n",
    "    :type device: str\n",
    "    :return: A tuple containing average loss and accuracy.\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    running_acc = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "        # Move the data to the desired training device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Perform the forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        # Compute accuracy metric\n",
    "        acc = compute_accuracy(outputs, labels)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform the backwards pass (i.e. backpropogate the error)\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add the running accuracy and loss\n",
    "        running_loss += loss.item()\n",
    "        running_acc += acc\n",
    "\n",
    "        # Progressively print loss\n",
    "        if (i+1) % 256 == 0:\n",
    "            print(\n",
    "                f'TRAINING --> Epoch: {epoch+1}/{num_epochs}, ' +\n",
    "                f'Step: {i+1}/{total_steps}, ' +\n",
    "                f'Loss: {running_loss / (i+1)}, '\n",
    "                f'Accuracy: {running_acc / (i+1)}'\n",
    "            )\n",
    "    # Compute the average loss and accuracy for this epoch\n",
    "    running_loss = running_loss / total_steps\n",
    "    running_acc = running_acc / total_steps\n",
    "\n",
    "    return running_loss, running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(\n",
    "    val_dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    loss_function,\n",
    "    epoch: int,\n",
    "    num_epochs: int,\n",
    "    total_steps: int,\n",
    "    device: str\n",
    ") -> tuple:\n",
    "    \"\"\"Perform inference over the entire validation dataset without training\n",
    "    the model.\n",
    "\n",
    "    :param val_dataloader: The validation dataset DataLoader to get data.\n",
    "    :type val_dataloader: torch.utils.data.DataLoader\n",
    "    :param model: The model to use to perform inference.\n",
    "    :type model: torch.nn.Module\n",
    "    :param loss_function: The loss function to use to compute the loss with.\n",
    "    :param epoch: The current epoch number.\n",
    "    :type epoch: int\n",
    "    :param num_epochs: Total number of epochs.\n",
    "    :type num_epochs: int\n",
    "    :param total_steps: Total number of loop iteration due to batching.\n",
    "    :type total_steps: int\n",
    "    :param device: The device to perform inference on.\n",
    "    :type device: str\n",
    "    :return: The average validation loss and accuracy.\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    # Now we run over the validation dataset without training\n",
    "    if val_dataloader:\n",
    "        # Disable the gradient calculations and updates\n",
    "        with torch.no_grad():\n",
    "            for i, (images, labels) in enumerate(val_dataloader):\n",
    "                # Transfer input data to device\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Perform inference\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Compute validation loss\n",
    "                loss = loss_function(outputs, labels)\n",
    "\n",
    "                # Compute validation accuracy\n",
    "                acc = compute_accuracy(outputs, labels)\n",
    "\n",
    "                # Add the running accuracy and loss\n",
    "                running_loss += loss.item()\n",
    "                running_acc += acc\n",
    "\n",
    "                # Progressively print loss\n",
    "                if (i+1) % 256 == 0:\n",
    "                    print(\n",
    "                        f'VALIDATION --> Epoch: {epoch+1}/{num_epochs}, ' +\n",
    "                        f'Step: {i+1}/{total_steps}, ' +\n",
    "                        f'Val Loss: {running_loss / (i+1)}, ' +\n",
    "                        f'Val Acc: {running_acc / (i+1)}'\n",
    "                    )\n",
    "    # Compute the average loss and accuracy for this epoch\n",
    "    running_loss = running_loss / total_steps\n",
    "    running_acc = running_acc / total_steps\n",
    "\n",
    "    return running_loss, running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(\n",
    "    model: torch.nn.Module,\n",
    "    model_save_path: Path,\n",
    "    val_loss: float,\n",
    "    val_losses: list,\n",
    "    epoch: int,\n",
    "    keep_models: bool = False\n",
    "):\n",
    "    \"\"\"Save the model if it is the first epoch. Subsequently, save the model\n",
    "    only if a lower validation loss is achieved whilst training.\n",
    "\n",
    "    :param model: The model to save.\n",
    "    :type model: torch.nn.Module\n",
    "    :param model_save_path: The location to save the model to.\n",
    "    :type model_save_path: Path\n",
    "    :param val_loss: The current epoch's validation loss.\n",
    "    :type val_loss: float\n",
    "    :param val_losses: The history of all other validation losses.\n",
    "    :type val_losses: list\n",
    "    :param epoch: The current epoch number.\n",
    "    :type epoch: int\n",
    "    :param keep_models: Should all models be saved, defaults to False\n",
    "    :type keep_models: bool, optional\n",
    "    \"\"\"\n",
    "    # Should we keep all models or just one\n",
    "    if keep_models:\n",
    "        model_save_path = model_save_path / f'model_{epoch+1}_{val_loss}.pt'\n",
    "    else:\n",
    "        model_save_path = model_save_path / f'model_state_dict.pt'\n",
    "    # Save the first model\n",
    "    if len(val_losses) == 0:\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            model_save_path\n",
    "        )\n",
    "        print(\n",
    "            'SAVING --> First epoch: \\n' +\n",
    "            f'Val Loss: {val_loss}\\n' +\n",
    "            f'Saving new model to:\\n{model_save_path}'\n",
    "        )\n",
    "    elif val_loss < min(val_losses):\n",
    "        # If our new validation loss is less than the previous best save the\n",
    "        # model\n",
    "        print(\n",
    "            'SAVING --> Found model with better validation loss: \\n' +\n",
    "            f'New Best Val Loss: {val_loss}\\n' +\n",
    "            f'Old Best Val Loss: {min(val_losses)}\\n'\n",
    "            f'Saving new model to:\\n{model_save_path}'\n",
    "        )\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            model_save_path\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    loss_function: torch.nn.CrossEntropyLoss,\n",
    "    optimizer: torch.optim.Adam,\n",
    "    num_epochs: int,\n",
    "    device: str,\n",
    "    model_save_path: Path = Path('./models'),\n",
    "    val_dataloader: torch.utils.data.DataLoader = None,\n",
    ") -> tuple:\n",
    "    \"\"\"Training loop which iterates over every image in the training dataset,\n",
    "    performs the forward pass, computes the loss, and then performs the\n",
    "    backwards pass. This also loops over the validation dataset and computes\n",
    "    the validation loss and accuracy to determine the best model.\n",
    "\n",
    "    See: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "    :param train_dataloader: The training dataset DataLoader to get data.\n",
    "    :type train_dataloader: torch.utils.data.DataLoader\n",
    "    :param model: The model to train.\n",
    "    :type model: torch.nn.Module\n",
    "    :param loss_function: The loss function to use to compute the loss.\n",
    "    :type loss_function: torch.nn.CrossEntropyLoss\n",
    "    :param optimizer: The optimizer to use in updating the weights.\n",
    "    :type optimizer: torch.optim.Adam\n",
    "    :param num_epochs: Total number of epoch to train for.\n",
    "    :type num_epochs: int\n",
    "    :param device: The device to perform training and validation on.\n",
    "    :type device: str\n",
    "    :param model_save_path: Save the model to, defaults to Path('./models')\n",
    "    :type model_save_path: Path, optional\n",
    "    :param val_dataloader: The validation dataset loader, defaults to None\n",
    "    :type val_dataloader: torch.utils.data.DataLoader, optional\n",
    "    :return: A tuple of shape (2, 2) that contains the history of the train\n",
    "        and validation losses and accuracies.\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    print(f'Models will be saved to: {model_save_path}')\n",
    "    # Lists for recording stats over epochs\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "    # Create the save path for the model\n",
    "    if not model_save_path.exists():\n",
    "        model_save_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Total number batches in training set\n",
    "    train_total_steps = len(train_dataloader)\n",
    "    val_total_steps = len(val_dataloader)\n",
    "\n",
    "    # Perform training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Enable model training\n",
    "        model.train(True)\n",
    "\n",
    "        # Enter the training function loop\n",
    "        train_loss, train_acc = train(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            num_epochs,\n",
    "            train_total_steps,\n",
    "            device\n",
    "        )\n",
    "        print(\n",
    "            f'TRAINING --> Epoch {epoch+1}/{NUM_EPOCHS} DONE, ' +\n",
    "            f'Avg Loss: {train_loss}, Avg Accuracy: {train_acc}'\n",
    "        )\n",
    "\n",
    "        # Enter the validation loop\n",
    "        val_loss, val_acc = validation(\n",
    "            val_dataloader,\n",
    "            model,\n",
    "            loss_function,\n",
    "            epoch,\n",
    "            num_epochs,\n",
    "            val_total_steps,\n",
    "            device\n",
    "        )\n",
    "        print(\n",
    "            f'VALIDATION --> Epoch {epoch+1}/{NUM_EPOCHS} DONE, ' +\n",
    "            f'Avg Loss: {val_loss}, Avg Accuracy: {val_acc}'\n",
    "        )\n",
    "\n",
    "        # Determine if we should save the model\n",
    "        save_best_model(model, model_save_path, val_loss, val_losses, epoch)\n",
    "\n",
    "        # Record the stats\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "    return (train_losses, train_accs), (val_losses, val_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q5: What is the purpose of the loss function?_**\n",
    "\n",
    "**_Q6: What is validation doing?_**\n",
    "\n",
    "**_Q7: Why do we perform validation and not just use the training loss/accuracy?_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epoch_metrics(x, y, data_names, title_prefix, yaxis_label):\n",
    "    \"\"\"Plot metrics with the number of epochs on the x axis and the metric of\n",
    "    interest on the y axis. Note that this function differs based on the input.\n",
    "\n",
    "    :param x: The values to use on the x-axis.\n",
    "    :type x: list\n",
    "    :param y: A list of lists containing len(x) data points to plot. The inner\n",
    "        lists are the different series to plot.\n",
    "    :type y: list\n",
    "    :param data_names: Names of the series to use in the legend.\n",
    "    :type data_names: str\n",
    "    :param title_prefix: A prefix to add before everything else in the title.\n",
    "    :type title_prefix: str\n",
    "    :param yaxis_label: The label for the y axis.\n",
    "    :type yaxis_label: str\n",
    "    \"\"\"\n",
    "    # Plot multiple series of data\n",
    "    for i in y:\n",
    "        plt.plot(x, i)\n",
    "    # Set the title\n",
    "    plt.title(title_prefix + ' ' + ' vs. '.join(data_names) + ' ' + yaxis_label)\n",
    "    # Set the y axis label\n",
    "    plt.ylabel(yaxis_label)\n",
    "    # Enable the legend with the appropriate names\n",
    "    plt.legend(data_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models will be saved to: models/vgg\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24955/394743340.py:106: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = torch.functional.F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([2, 64])\n",
      "TRAINING --> Epoch 1/10 DONE, Avg Loss: 2.312145109176636, Avg Accuracy: 0.09125\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([28, 64])\n",
      "VALIDATION --> Epoch 1/10 DONE, Avg Loss: 2.3104799134390697, Avg Accuracy: 0.10012755117246083\n",
      "SAVING --> First epoch: \n",
      "Val Loss: 2.3104799134390697\n",
      "Saving new model to:\n",
      "models/vgg/model_state_dict.pt\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([2, 64])\n",
      "TRAINING --> Epoch 2/10 DONE, Avg Loss: 2.3147641086578368, Avg Accuracy: 0.1025\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([28, 64])\n",
      "VALIDATION --> Epoch 2/10 DONE, Avg Loss: 2.3154314926692416, Avg Accuracy: 0.09693877611841474\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([2, 64])\n",
      "TRAINING --> Epoch 3/10 DONE, Avg Loss: 2.305225315093994, Avg Accuracy: 0.105\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([28, 64])\n",
      "VALIDATION --> Epoch 3/10 DONE, Avg Loss: 2.305685179574149, Avg Accuracy: 0.09502551066023963\n",
      "SAVING --> Found model with better validation loss: \n",
      "New Best Val Loss: 2.305685179574149\n",
      "Old Best Val Loss: 2.3104799134390697\n",
      "Saving new model to:\n",
      "models/vgg/model_state_dict.pt\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([2, 64])\n",
      "TRAINING --> Epoch 4/10 DONE, Avg Loss: 2.2998892784118654, Avg Accuracy: 0.125\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([28, 64])\n",
      "VALIDATION --> Epoch 4/10 DONE, Avg Loss: 2.2956935337611606, Avg Accuracy: 0.10395408208881106\n",
      "SAVING --> Found model with better validation loss: \n",
      "New Best Val Loss: 2.2956935337611606\n",
      "Old Best Val Loss: 2.305685179574149\n",
      "Saving new model to:\n",
      "models/vgg/model_state_dict.pt\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([2, 64])\n",
      "TRAINING --> Epoch 5/10 DONE, Avg Loss: 2.2785954380035403, Avg Accuracy: 0.1025\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([28, 64])\n",
      "VALIDATION --> Epoch 5/10 DONE, Avg Loss: 2.275724172592163, Avg Accuracy: 0.10331632675869125\n",
      "SAVING --> Found model with better validation loss: \n",
      "New Best Val Loss: 2.275724172592163\n",
      "Old Best Val Loss: 2.2956935337611606\n",
      "Saving new model to:\n",
      "models/vgg/model_state_dict.pt\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([2, 64])\n",
      "TRAINING --> Epoch 6/10 DONE, Avg Loss: 2.2703470039367675, Avg Accuracy: 0.14875\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([28, 64])\n",
      "VALIDATION --> Epoch 6/10 DONE, Avg Loss: 2.2885751724243164, Avg Accuracy: 0.13137755117246083\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([2, 64])\n",
      "TRAINING --> Epoch 7/10 DONE, Avg Loss: 2.274230709075928, Avg Accuracy: 0.13125\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([28, 64])\n",
      "VALIDATION --> Epoch 7/10 DONE, Avg Loss: 2.283034324645996, Avg Accuracy: 0.13201530703476497\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([2, 64])\n",
      "TRAINING --> Epoch 8/10 DONE, Avg Loss: 2.2609206867218017, Avg Accuracy: 0.15\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([28, 64])\n",
      "VALIDATION --> Epoch 8/10 DONE, Avg Loss: 2.2589358602251326, Avg Accuracy: 0.13711734754698618\n",
      "SAVING --> Found model with better validation loss: \n",
      "New Best Val Loss: 2.2589358602251326\n",
      "Old Best Val Loss: 2.275724172592163\n",
      "Saving new model to:\n",
      "models/vgg/model_state_dict.pt\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([2, 64])\n",
      "TRAINING --> Epoch 9/10 DONE, Avg Loss: 2.2616116428375244, Avg Accuracy: 0.13375\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([28, 64])\n",
      "VALIDATION --> Epoch 9/10 DONE, Avg Loss: 2.2573654311043874, Avg Accuracy: 0.1224489797438894\n",
      "SAVING --> Found model with better validation loss: \n",
      "New Best Val Loss: 2.2573654311043874\n",
      "Old Best Val Loss: 2.2589358602251326\n",
      "Saving new model to:\n",
      "models/vgg/model_state_dict.pt\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([2, 64])\n",
      "TRAINING --> Epoch 10/10 DONE, Avg Loss: 2.2525892639160157, Avg Accuracy: 0.145\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([32, 64])\n",
      "shape of flat vector: torch.Size([28, 64])\n",
      "VALIDATION --> Epoch 10/10 DONE, Avg Loss: 2.2516442026410783, Avg Accuracy: 0.14795918390154839\n",
      "SAVING --> Found model with better validation loss: \n",
      "New Best Val Loss: 2.2516442026410783\n",
      "Old Best Val Loss: 2.2573654311043874\n",
      "Saving new model to:\n",
      "models/vgg/model_state_dict.pt\n",
      "Best Validation Loss: 2.2516442026410783 after epoch 10\n",
      "Best Validation Acc: 0.14795918390154839 after epoch 10\n"
     ]
    }
   ],
   "source": [
    "# Train the VGG Model\n",
    "vgg_model.to(DEVICE)\n",
    "(train_losses, train_accs), (val_losses, val_accs) = train_model(\n",
    "    train_dataloader,\n",
    "    vgg_model,\n",
    "    loss,\n",
    "    optim_vgg,\n",
    "    NUM_EPOCHS,\n",
    "    DEVICE,\n",
    "    model_save_path=Path('./models/vgg'),\n",
    "    val_dataloader=val_dataloader\n",
    ")\n",
    "# Get the best validation loss and accuracy\n",
    "print(\n",
    "    f'Best Validation Loss: {min(val_losses)} ' +\n",
    "    f'after epoch {np.argmin(val_losses) + 1}'\n",
    ")\n",
    "print(\n",
    "    f'Best Validation Acc: {max(val_accs)} ' +\n",
    "    f'after epoch {np.argmax(val_accs) + 1}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epoch_metrics(\n",
    "    np.arange(10),\n",
    "    [train_losses, val_losses],\n",
    "    ['Train', 'Validation'],\n",
    "    'Original LeNet-5',\n",
    "    'Loss'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epoch_metrics(\n",
    "    np.arange(10),\n",
    "    [train_accs, val_accs],\n",
    "    ['Train', 'Validation'],\n",
    "    'Original LeNet-5',\n",
    "    'Accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q8: Do you think training the models for additional epochs will help in any way? Explain your answer!_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our model and chosen the best model based on the validation loss, let's test our saved models and compare them in terms of their loss and accuracy on the test dataset. If the models have been trained well, we should see they perform decently well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(labels, predictions):\n",
    "    \"\"\"This function generates a confusion matrix which shows how the model\n",
    "    performs over the different classes in the dataset. In other words, this\n",
    "    shows which classes the model 'confuses' with one another.\n",
    "\n",
    "    :param labels: The ground-truth labels.\n",
    "    :type labels: list\n",
    "    :param predictions: The predictions provided by the model.\n",
    "    :type predictions: list\n",
    "    \"\"\"\n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    # Create a matplotlib display for the confusion matrix\n",
    "    cm_display = ConfusionMatrixDisplay(cm, display_labels=np.arange(10))\n",
    "    # Plot the data\n",
    "    cm_display.plot()\n",
    "    # SHow the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    model_class: torch.nn.Module,\n",
    "    model_weights_path: Path,\n",
    "    device: str,\n",
    "    loss_function\n",
    "):\n",
    "    \"\"\"Load the saved model and evaluate it using the test dataset to determine\n",
    "    how well it was trained.\n",
    "\n",
    "    :param test_dataloader: The test dataset DataLoader to get the data from.\n",
    "    :type test_dataloader: torch.utils.data.DataLoader\n",
    "    :param model_class: The model to evaluate.\n",
    "    :type model_class: torch.nn.Module\n",
    "    :param model_weights_path: The path the the model's weights.\n",
    "    :type model_weights_path: Path\n",
    "    :param device: The device to use for evaluation (i.e. inference).\n",
    "    :type device: str\n",
    "    :param loss_function: The loss function to use to compute the test loss.\n",
    "    :return: A tuple containing the total average loss and accuracy as well as\n",
    "        two list containing the predictions and ground-truth labels.\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    # Initialize the model architecture\n",
    "    model: torch.nn.Module = model_class()\n",
    "    # Load the weights\n",
    "    model.load_state_dict(torch.load(model_weights_path, weights_only=True))\n",
    "    # Enter the model into evaluation mode (i.e. DO NOT TRAIN)\n",
    "    model.eval()\n",
    "\n",
    "    # Send the model to the device to use for inference\n",
    "    model.to(device)\n",
    "    # Number of batched iterations\n",
    "    total_steps = len(test_dataloader)\n",
    "    # Vars for metrics and results\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    # Testing/Evaluation loop\n",
    "    for i, (images, labels) in enumerate(test_dataloader):\n",
    "        # Send data to the inference device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Perform inference on batched data\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute the loss (error) between the output and the ground-truths\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        # Compute the accuracy\n",
    "        accuracy = compute_accuracy(outputs, labels)\n",
    "\n",
    "        # Keep a running loss and accuracy for computing the average\n",
    "        running_loss += loss.item()\n",
    "        running_acc += accuracy\n",
    "\n",
    "        # Concatenate all of the predictions and ground-truths\n",
    "        all_predictions = all_predictions + torch.argmax(outputs, 1).tolist()\n",
    "        all_labels = all_labels + labels.tolist()\n",
    "    # Compute the average loss and accuracy\n",
    "    running_loss = running_loss / total_steps\n",
    "    running_acc = running_acc / total_steps\n",
    "\n",
    "    return running_loss, running_acc, all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_test_loss, og_test_acc, predictions, labels = test_model(\n",
    "    test_dataloader,\n",
    "    OGLeNet5,\n",
    "    './models/original_lenet5/model_state_dict.pt',\n",
    "    DEVICE,\n",
    "    loss\n",
    ")\n",
    "print('Original LeNet-5 Inference Performance')\n",
    "print(f'Test dataset contains {len(test_dataset)} samples.')\n",
    "print(f'Testing Avg Loss: {og_test_loss}')\n",
    "print(f'Testing Avg Acc: {og_test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_test_loss, modern_test_acc, predictions, labels = test_model(\n",
    "    test_dataloader,\n",
    "    ModernLeNet5,\n",
    "    './models/modern_lenet5/model_state_dict.pt',\n",
    "    DEVICE,\n",
    "    loss\n",
    ")\n",
    "print('Modern LeNet-5 Inference Performance')\n",
    "print(f'Test dataset contains {len(test_dataset)} samples.')\n",
    "print(f'Testing Avg Loss: {modern_test_loss}')\n",
    "print(f'Testing Avg Acc: {modern_test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q9: How does the test loss and accuracy compare to the best training and validation metrics?_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Q10: How do you think we could compute the latency inside the `test_model` function? In other words, what code do we add to `test_model` to measure the time it takes to give the model input and then get the output?_**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
